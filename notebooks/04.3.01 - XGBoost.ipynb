{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb693771",
   "metadata": {},
   "source": [
    "<div style=\" border-bottom: 8px solid #e3f56c; overflow: hidden; border-radius: 10px; height: 60px; width: 100%; display: flex;\">\n",
    "  <div style=\"height: 100%; width: 100%; background-color: #3800BB; float: left; text-align: center; display: flex; justify-content: left; align-items: center; font-size: 40px; \">\n",
    "    <b><span style=\"color: #FFFFFF; padding: 20px 20px;\">XGBoost</span></b>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cbe06",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color: #FEDAD5; border-left: 8px solid #B12111; padding: 14px; border-radius: 8px; font-size: 14px; color: #000000;\">\n",
    "  <h5 style=\"font-size: 20px; margin-bottom: 10px;\">\n",
    "    <strong> Contents </strong>\n",
    "  </h5>\n",
    "<hr>\n",
    "  <p><font size=\"3\" face=\"Arial\" font-size=\"large\">\n",
    "  <ul type=\"square\">\n",
    "      \n",
    "- `DMatrix` for data preprocessing  \n",
    "- Two main types of model training in XGBoost  \n",
    "- Important model hyperparameters  \n",
    "- Hyperparameter tuning  \n",
    "- `LearningRateScheduler`  \n",
    "- Custom metrics from `scikit-learn`  \n",
    "- Conclusions and summary  \n",
    "\n",
    "\n",
    "  </ul>\n",
    "  </font></p>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/800/1*l4PN8hyAO4fMLxUbIxcETA.png' align=\"left\" width=65% height=65% />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e5314",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:  #E8F8F5; border-left: 8px solid #1ABC9C; padding: 14px; border-radius: 8px; font-size: 14px; color: #000000;\">\n",
    "\n",
    "`XGBoost` was developed as a research project at the University of Washington. Tianqi Chen and Carlos Guestrin presented their work at the SIGKDD conference in 2016, making a big impact in the machine learning world. Since its release, this algorithm has not only dominated Kaggle competitions but has also powered numerous industrial applications.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd8013",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### **Some key features:**\n",
    "</div>\n",
    "\n",
    "<div class=\"alert\" style=\"background-color:  #E8F8F5; border-left: 8px solid #1ABC9C; padding: 14px; border-radius: 8px; font-size: 14px; color: #000000;\">\n",
    "\n",
    "* Rich set of regularization parameters (`lambda` / `gamma` / `alpha`)\n",
    "* Considered one of the fastest and least resource-intensive boosting algorithms (though debatable)\n",
    "* Built-in handling of missing values and categorical features (via `one-hot encoding`) (`enable_categorical = True` / `df[cat_col].astype('category')`)\n",
    "* Native support for metrics from **scikit-learn** as custom metrics\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8176279",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:  #E8F8F5; border-left: 8px solid #1ABC9C; padding: 14px; border-radius: 8px; font-size: 14px; color: #000000;\">\n",
    "\n",
    "* Native support for `Dask` and `Spark`\n",
    "* Compatible with popular cloud platforms and supports distributed training across clusters or multiple machines\n",
    "* Supports most major programming languages\n",
    "* Integrates well with other frameworks and services (**Optuna**, **Weights & Biases**, etc.)\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb868a4",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:  #E8F8F5; border-left: 8px solid #1ABC9C; padding: 14px; border-radius: 8px; font-size: 14px; color: #000000;\">\n",
    "\n",
    "**Boosting Implementation Details (see the diagram):**\n",
    "\n",
    "* **Parallelization:** In `XGBoost`, tree construction is parallelized. This is made possible by rearranging the nested loops used to compute split points: the outer loop iterates over tree leaves, and the inner one over features. Swapping the loops allows efficient parallel computation. Initialization occurs during data loading, followed by sorting with parallel threads, which significantly speeds up training.\n",
    "\n",
    "* **Tree Pruning (`gamma`):** Unlike traditional GBMs that use negative loss reduction as a split-stopping criterion, `XGBoost` employs `max_depth` and performs *post-pruning* based on a minimum loss reduction (`gamma`). This approach improves computational efficiency.\n",
    "\n",
    "* **Hardware Optimization:** The algorithm is engineered to maximize hardware utilization by creating internal buffers per thread to store gradient statistics.\n",
    "\n",
    "* **Regularization:** To prevent overfitting, `XGBoost` applies both L1 (LASSO) and L2 (Ridge) regularization.\n",
    "\n",
    "* **Handling Missing Values:** `XGBoost` handles sparse data gracefully, learning how to assign missing values based on the direction that minimizes loss.\n",
    "\n",
    "* **Cross-Validation:** The algorithm includes a built-in method for performing cross-validation during training.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b84a9",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:  #E8F8F5; border-left: 8px solid #1ABC9C; padding: 14px; border-radius: 8px; font-size: 14px; color: #000000;\">\n",
    "\n",
    "From the userâ€™s perspective, `XGBoost` is quite similar to `LightGBM`: it uses a `booster` object, has a dedicated **sklearn API**, supports `dart`, `callbacks`, `scheduler`, etc. However, there are some key differences worth exploring.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b08354",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color: #FEF9E7; border-left: 8px solid #D4AC0D; padding: 14px; border-radius: 8px; font-size: 14px; color: #000000;\">\n",
    "\n",
    "!pip install xgboost\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bff220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
